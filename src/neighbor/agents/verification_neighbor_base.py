"""Base verification agent for Neighbor profiles using Gemini Deep Research.

This module provides the base class for verifying neighbor profiles using
Google's Gemini Deep Research API. It handles the common polling and
response parsing logic.
"""

import os
import re
import json
import time
import subprocess
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional

from google import genai

from ..config.settings import settings
from ..config.prompts import VERIFICATION_NEIGHBOR_SYSTEM


class NeighborVerificationAgent:
    """Base class for neighbor verification using Gemini Deep Research."""

    def __init__(self):
        """Initialize the Gemini client."""
        api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
        if not api_key:
            raise ValueError(
                "GEMINI_API_KEY or GOOGLE_API_KEY environment variable required"
            )
        self.client = genai.Client(api_key=api_key)
        self.name = "Verification Agent - Neighbor (Gemini Deep Research)"
        self.agent = "deep-research-pro-preview-12-2025"
        self.poll_interval = settings.GEMINI_POLL_INTERVAL
        self.max_wait_time = settings.GEMINI_MAX_WAIT_TIME

    def _get_system_prompt(self) -> str:
        """Get the system prompt for verification.

        Override in subclasses to add entity-specific prompts.
        """
        return VERIFICATION_NEIGHBOR_SYSTEM

    def _build_verification_input(
        self,
        profiles: List[Dict[str, Any]],
        context: Dict[str, Any],
        entity_type: str,
    ) -> str:
        """Build the full input for Gemini Deep Research.

        Args:
            profiles: List of neighbor profile dicts to verify
            context: Dict with county, state, city
            entity_type: "person" or "organization"

        Returns:
            Formatted input string for Gemini
        """
        county = context.get("county", "Unknown")
        state = context.get("state", "Unknown")
        current_date = datetime.now().strftime("%Y-%m-%d")

        # Format system prompt with context
        system_prompt = self._get_system_prompt().format(
            county=county,
            state=state,
            entity_type=entity_type,
        )

        # Format profiles as JSON
        profiles_json = json.dumps(profiles, indent=2, ensure_ascii=False)

        full_input = f"""{system_prompt}

---

## Verification Context
- County: {county}
- State: {state}
- Entity Type: {entity_type}
- Current Date: {current_date}
- Number of Profiles: {len(profiles)}

## Draft Profiles to Verify
```json
{profiles_json}
```

## Instructions
1. Verify ALL profiles above using web search
2. Apply the EDITING POLICY strictly: fix in place, remove incorrect data, mark "unknown" for unverifiable
3. Ensure every claim has a valid URL citation
4. Return the verified profiles in the EXACT same JSON structure

CRITICAL: Your output MUST be a valid JSON array of neighbor profiles wrapped in ```json code blocks.
"""
        return full_input

    def verify_batch(
        self,
        profiles: List[Dict[str, Any]],
        context: Dict[str, Any],
        entity_type: str = "person",
        source_file: Optional[str] = None,
    ) -> Dict[str, Any]:
        """Verify a batch of neighbor profiles.

        Args:
            profiles: List of neighbor profile dicts to verify
            context: Dict with county, state, city
            entity_type: "person" or "organization"
            source_file: Name of the source dr_*.json file (for DEBUG matching)

        Returns:
            Dict with verified profiles and metadata
        """
        county = context.get("county", "Unknown")
        state = context.get("state", "Unknown")

        print(f"\n{'=' * 70}")
        print(f"ðŸ” GEMINI DEEP RESEARCH - NEIGHBOR VERIFICATION ({entity_type.upper()})")
        print(f"{'=' * 70}")
        print(f"   Location: {county}, {state}")
        print(f"   Entity type: {entity_type}")
        print(f"   Profiles to verify: {len(profiles)}")
        print(f"   Source file: {source_file or 'N/A'}")
        print(f"   Agent: {self.agent}")

        if not profiles:
            return {
                "status": "completed",
                "neighbors": [],
                "metadata": {"note": "No profiles to verify"},
            }

        # Check for existing DEBUG JSON files for THIS source file to re-parse
        debug_dir = Path(__file__).parent.parent / "deep_research_outputs"
        debug_files = sorted(debug_dir.glob("interaction_DEBUG_*.json"), reverse=True) if debug_dir.exists() else []

        if debug_files and source_file:
            print(f"   ðŸ“‚ Found {len(debug_files)} DEBUG JSON file(s), looking for match to {source_file}...")
            for debug_file in debug_files:
                try:
                    with open(debug_file, "r", encoding="utf-8") as f:
                        debug_data = json.load(f)

                    # Verify this DEBUG file matches current source file
                    if debug_data.get("source_file") != source_file:
                        continue

                    print(f"   ðŸ”„ Found matching DEBUG: {debug_file.name}")

                    # Extract verified_content from debug JSON
                    verified_content = None
                    thinking_summaries = []

                    for output in debug_data.get("outputs", []):
                        output_type = output.get("type")
                        if output_type == 'thought' and output.get("summary"):
                            summary = output["summary"]
                            if isinstance(summary, list):
                                for item in summary:
                                    if isinstance(item, dict) and item.get("text"):
                                        thinking_summaries.append(item["text"])
                                    elif isinstance(item, str):
                                        thinking_summaries.append(item)
                            elif isinstance(summary, str):
                                thinking_summaries.append(summary)
                        elif output_type == 'text' and output.get("text"):
                            verified_content = output["text"]

                    if not verified_content and debug_data.get("outputs"):
                        verified_content = debug_data["outputs"][-1].get("text")

                    if verified_content:
                        print(f"   ðŸ“Š Parsing cached content: {len(verified_content)} chars")
                        verified_profiles = self._parse_json_output(verified_content, profiles)

                        # Success - delete the DEBUG file since it's been parsed
                        subprocess.run(["trash", str(debug_file)], check=True)
                        print(f"   ðŸ—‘ï¸ Trashed parsed DEBUG file: {debug_file.name}")

                        print(f"\n{'=' * 70}")
                        print("âœ… NEIGHBOR VERIFICATION COMPLETE (from cached DEBUG)")
                        print(f"{'=' * 70}")
                        print(f"   Profiles verified: {len(verified_profiles)}")

                        return {
                            "status": "completed",
                            "neighbors": verified_profiles,
                            "metadata": {
                                "thinking_summaries": thinking_summaries,
                                "total_tokens": 0,
                                "entity_type": entity_type,
                                "profiles_input": len(profiles),
                                "profiles_output": len(verified_profiles),
                                "from_cache": True,
                            },
                        }
                except Exception as e:
                    print(f"   âš ï¸ Failed to parse {debug_file.name}: {e}")
                    continue

            print(f"   â„¹ï¸ No matching DEBUG file for {source_file}, proceeding with API call...")

        # Build full input
        full_input = self._build_verification_input(profiles, context, entity_type)
        print(f"   ðŸ“„ Input size: {len(full_input)} characters")

        # Retry loop for verification
        max_retries = settings.VERIFICATION_MAX_RETRIES
        retry_delay = settings.VERIFICATION_RETRY_DELAY
        last_error = None
        debug_json_path = None  # Track for cleanup on retry

        for attempt in range(max_retries + 1):
            is_final_attempt = (attempt == max_retries)

            if attempt > 0:
                print(f"   ðŸ”„ Retry {attempt}/{max_retries} after {retry_delay}s (reason: {last_error})")
                time.sleep(retry_delay)

            try:
                # Clean up any DEBUG file from previous failed attempt
                if debug_json_path and debug_json_path.exists():
                    subprocess.run(["trash", str(debug_json_path)], check=False)
                    print(f"   ðŸ—‘ï¸ Cleaned up DEBUG file from failed attempt: {debug_json_path.name}")
                    debug_json_path = None

                # Start Deep Research task
                print(f"   ðŸš€ Sending to Gemini Deep Research API...")

                interaction = self.client.interactions.create(
                    input=full_input,
                    agent=self.agent,
                    background=True,
                    store=True,  # Required when using background=True
                    agent_config={
                        "type": "deep-research",
                        "thinking_summaries": "auto",
                    },
                )

                interaction_id = interaction.id
                print(f"   ðŸ“¡ Interaction started with ID: {interaction_id}")
                print(f"   â³ Polling for completion (max {self.max_wait_time // 60} minutes)...")

                # Poll for completion
                start_time = time.time()
                last_status = None

                while True:
                    elapsed = time.time() - start_time
                    if elapsed > self.max_wait_time:
                        last_error = f"Timeout after {self.max_wait_time // 60} minutes"
                        if is_final_attempt:
                            return {
                                "status": "failed",
                                "error": last_error,
                                "neighbors": profiles,
                            }
                        break  # Exit polling loop to retry

                    interaction = self.client.interactions.get(interaction_id)
                    status = interaction.status

                    if status != last_status:
                        print(f"   ðŸ“Š Status: {status} ({int(elapsed)}s elapsed)")
                        last_status = status

                    if status == "completed":
                        break
                    elif status in ["failed", "cancelled"]:
                        error_msg = getattr(interaction, "error", None) or f"Status: {status}"
                        last_error = str(error_msg)
                        if is_final_attempt:
                            return {
                                "status": "failed",
                                "error": last_error,
                                "neighbors": profiles,
                            }
                        break  # Exit polling loop to retry

                    time.sleep(self.poll_interval)

                # If we broke out of polling due to error, continue to next retry attempt
                if last_error and status != "completed":
                    continue

                # Save full interaction response as debug JSON before extracting
                debug_json_path = Path(__file__).parent.parent / "deep_research_outputs" / f"interaction_DEBUG_{entity_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                debug_json_path.parent.mkdir(parents=True, exist_ok=True)
                try:
                    # Serialize full interaction response (use model_dump if available)
                    if hasattr(interaction, 'model_dump'):
                        debug_data = interaction.model_dump()
                    else:
                        debug_data = {attr: getattr(interaction, attr) for attr in dir(interaction)
                                      if not attr.startswith('_') and not callable(getattr(interaction, attr))}

                    # Add our tracking fields
                    debug_data["source_file"] = source_file
                    debug_data["entity_type"] = entity_type

                    with open(debug_json_path, "w", encoding="utf-8") as f:
                        json.dump(debug_data, f, indent=2, ensure_ascii=False, default=str)
                    print(f"   ðŸ“„ Saved interaction debug to: {debug_json_path.name}")
                except Exception as e:
                    print(f"   âš ï¸ Failed to save interaction debug: {e}")
                    last_error = f"Failed to save interaction debug: {e}"
                    if is_final_attempt:
                        return {
                            "status": "failed",
                            "error": last_error,
                            "neighbors": profiles,
                        }
                    continue

                # Extract output ONLY from the saved debug JSON file
                with open(debug_json_path, "r", encoding="utf-8") as f:
                    debug_data = json.load(f)

                if not debug_data.get("outputs"):
                    last_error = "No outputs in completed interaction"
                    if is_final_attempt:
                        return {
                            "status": "failed",
                            "error": last_error,
                            "neighbors": profiles,
                        }
                    continue

                # Separate thinking summaries from text output (from debug JSON)
                thinking_summaries = []
                verified_content = None

                for output in debug_data.get("outputs", []):
                    output_type = output.get("type")
                    if output_type == 'thought' and output.get("summary"):
                        summary = output["summary"]
                        if isinstance(summary, list):
                            for item in summary:
                                # Handle both string and text content object formats
                                if isinstance(item, dict) and item.get("text"):
                                    thinking_summaries.append(item["text"])
                                elif isinstance(item, str):
                                    thinking_summaries.append(item)
                        elif isinstance(summary, str):
                            thinking_summaries.append(summary)
                    elif output_type == 'text' and output.get("text"):
                        verified_content = output["text"]

                # Fallback to last output if no text type found
                if not verified_content and debug_data["outputs"]:
                    verified_content = debug_data["outputs"][-1].get("text")

                if not verified_content:
                    last_error = "Empty output from Gemini Deep Research"
                    if is_final_attempt:
                        return {
                            "status": "failed",
                            "error": last_error,
                            "neighbors": profiles,
                        }
                    continue

                # Log what we're about to parse (should match debug JSON exactly)
                print(f"   ðŸ“Š Parsing content from debug JSON: {len(verified_content)} chars")
                print(f"   ðŸ“„ Source: {debug_json_path.name}")

                # Parse verified profiles from output
                try:
                    verified_profiles = self._parse_json_output(verified_content, profiles)
                except ValueError as e:
                    # JSON parse failure - only save raw .txt on final failure
                    last_error = f"Parse error: {e}"
                    if is_final_attempt:
                        self._save_raw_debug(verified_content, str(e))
                        return {
                            "status": "failed",
                            "error": last_error,
                            "neighbors": profiles,
                        }
                    # Non-final attempt: just retry (no .txt created, nothing to clean up)
                    continue

                # Success - trash the DEBUG file (no longer needed, vr_* will be created)
                if debug_json_path and debug_json_path.exists():
                    subprocess.run(["trash", str(debug_json_path)], check=False)
                    print(f"   ðŸ—‘ï¸ Trashed DEBUG file after successful parse: {debug_json_path.name}")

                # Get usage info if available
                total_tokens = 0
                if hasattr(interaction, "usage") and interaction.usage:
                    total_tokens = getattr(interaction.usage, "total_tokens", 0)

                print(f"\n{'=' * 70}")
                print("âœ… NEIGHBOR VERIFICATION COMPLETE")
                print(f"{'=' * 70}")
                print(f"   Profiles verified: {len(verified_profiles)}")
                print(f"   Thinking summaries: {len(thinking_summaries)}")
                if total_tokens:
                    print(f"   Total tokens: {total_tokens}")
                if attempt > 0:
                    print(f"   âœ“ Succeeded after {attempt} retry(ies)")

                return {
                    "status": "completed",
                    "neighbors": verified_profiles,
                    "metadata": {
                        "thinking_summaries": thinking_summaries,
                        "total_tokens": total_tokens,
                        "entity_type": entity_type,
                        "profiles_input": len(profiles),
                        "profiles_output": len(verified_profiles),
                        "retry_count": attempt,
                    },
                }

            except Exception as e:
                last_error = str(e)
                print(f"   âŒ Attempt {attempt + 1} failed: {e}")
                if is_final_attempt:
                    import traceback
                    traceback.print_exc()
                    return {
                        "status": "failed",
                        "error": last_error,
                        "neighbors": profiles,
                    }
                continue

        # Should not reach here, but safety return
        return {
            "status": "failed",
            "error": f"Failed after {max_retries} retries: {last_error}",
            "neighbors": profiles,
        }

    def _parse_json_output(
        self,
        content: str,
        original_profiles: List[Dict[str, Any]],
    ) -> List[Dict[str, Any]]:
        """Parse JSON profiles from Gemini output.

        Args:
            content: Raw text output from Gemini
            original_profiles: Original profiles to fall back to

        Returns:
            List of verified profile dicts
        """
        # Clean up content
        content = content.strip()

        # Remove markdown code block wrapper if present
        if content.startswith("```json"):
            content = content[len("```json"):].strip()
        elif content.startswith("```"):
            first_newline = content.find("\n")
            if first_newline != -1:
                content = content[first_newline + 1:].strip()

        if content.endswith("```"):
            content = content[:-3].strip()

        # Try to find JSON array or object in content
        # Strategy 1: Find ```json code blocks and extract the full JSON (greedy)
        json_block_match = re.search(r'```json\s*(\[[\s\S]*\])\s*```', content)
        if json_block_match:
            json_str = json_block_match.group(1)
            try:
                parsed = json.loads(json_str)
                if isinstance(parsed, list):
                    print(f"   âœ“ Parsed {len(parsed)} profiles from JSON code block")
                    return parsed
            except json.JSONDecodeError:
                pass

        # Strategy 2: Try regex patterns
        json_patterns = [
            # Pattern 1: Direct JSON array (entire content)
            r'^\s*\[[\s\S]*\]\s*$',
            # Pattern 2: Object with neighbors array
            r'\{\s*"neighbors"\s*:\s*(\[[\s\S]*?\])',
            # Pattern 3: Find array starting with neighbor_id (greedy to get full array)
            r'(\[\s*\{[^[]*"neighbor_id"[\s\S]*\])',
        ]

        for pattern in json_patterns:
            match = re.search(pattern, content)
            if match:
                json_str = match.group(1) if match.lastindex else match.group(0)
                try:
                    parsed = json.loads(json_str)
                    if isinstance(parsed, list):
                        print(f"   âœ“ Parsed {len(parsed)} profiles from JSON array")
                        return parsed
                    elif isinstance(parsed, dict) and "neighbors" in parsed:
                        print(f"   âœ“ Parsed {len(parsed['neighbors'])} profiles from neighbors key")
                        return parsed["neighbors"]
                except json.JSONDecodeError:
                    continue

        # Try parsing entire content as JSON
        try:
            parsed = json.loads(content)
            if isinstance(parsed, list):
                return parsed
            elif isinstance(parsed, dict) and "neighbors" in parsed:
                return parsed["neighbors"]
        except json.JSONDecodeError:
            pass

        # FAIL - raise error (caller decides whether to save debug file)
        print(f"   âŒ Could not parse JSON from Gemini output!")
        print(f"   Content length: {len(content)} chars")
        print(f"   First 200 chars: {content[:200]}...")

        raise ValueError(f"Failed to parse JSON from Gemini output ({len(content)} chars)")

    def _save_raw_debug(self, content: str, error: str) -> Path:
        """Save raw output for debugging parse failures.

        Args:
            content: The raw content that failed to parse
            error: The error message

        Returns:
            Path to the saved debug file
        """
        debug_path = (
            Path(__file__).parent.parent
            / "deep_research_outputs"
            / f"gemini_raw_output_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        )
        debug_path.parent.mkdir(parents=True, exist_ok=True)
        with open(debug_path, "w", encoding="utf-8") as f:
            f.write(f"=== GEMINI RAW OUTPUT ===\n")
            f.write(f"Parse error: {error}\n")
            f.write(f"Length: {len(content)} chars\n")
            f.write(f"First 500 chars:\n{content[:500]}\n\n")
            f.write(f"Last 500 chars:\n{content[-500:]}\n\n")
            f.write(f"=== FULL CONTENT ===\n{content}")
        print(f"   ðŸ“„ Raw output saved to: {debug_path.name}")
        return debug_path
